{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3469443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (20.0.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2020.12.5)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.4.7)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a95d6",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 \n",
    "jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the \n",
    "location” field. \n",
    "3. Then click the search button. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b3e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e9be8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3439dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b79f15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de8c9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36120876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "# job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b91b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4f115bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cdfe8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-analytics-jobs-in-bangalore-bengaluru?k=data%20analytics&l=bangalore%2Fbengaluru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48ed4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617507a",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484efa79",
   "metadata": {},
   "source": [
    "So,Now lets first create 10 empty lists. In these lists the data will be stored while scraping. We have created 10 empty lists for 4 features we have to extract "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad34b8",
   "metadata": {},
   "source": [
    "1.job_titles2.company_names3.locations_list4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dadd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e88e0",
   "metadata": {},
   "source": [
    "First,we will extract all the tags where we have the jobs titles.Let me first show you on the webpae in which tags the job titles are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90d8847d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"c30ea6bb-f64f-469f-aeac-3972bfecb1c8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"10433c6d-0b7a-4c5b-9cdf-9a9b71c5b649\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"6d7b0926-c7c2-45f6-8147-86e9dc0d8e46\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"235c98b1-a722-4775-ab6d-473aa38da511\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"8a95b47e-9a9e-4b81-8dfa-901267d0a66b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"7046f061-72da-430c-ad0c-b1cf3a7e0f8a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"34b7d8d7-80e2-463c-857a-7f59d56c64dc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"cfafc7cd-d27d-4767-b424-e8a4e16babe9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"2b177f66-4489-4b41-b559-baec36a53509\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"ef3e1117-6a9e-453e-8d39-6f71c897c744\")>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the jobs-titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd2646",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6d030",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0975661a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Analyst - Flipkart Analytics',\n",
       " 'Data Analytics and Interpretation Application Developer',\n",
       " 'Manager - Data Analytics',\n",
       " 'Data Analytics Manager - Machine Learning Algorithms',\n",
       " 'Data Analytics - Pharma',\n",
       " 'Manager - Data Analytics',\n",
       " 'Big Data Developer - Data Analytics',\n",
       " 'Urgent requirement For Data analytics.',\n",
       " 'S&PP COE Analytics - Data Engineering',\n",
       " 'ACN - Digital - Analytics - Big Data - 10']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "# So we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eee9c",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the company names. Lets me first show you in which tags the company names are put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14f70ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"da644a94-1ada-4f31-9f7b-24dbba835372\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"a04073ba-e240-49d7-a074-c07ba4331fa1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"d1d5740d-387d-4f71-8fd3-54a24d1c64ad\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"935af4dc-0781-4d3a-9061-7bc8c7cf2647\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"bfef4629-1c12-49d4-aed6-29efd7551041\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"35bd5377-92ac-421e-a0fa-8644ac4efd40\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"c3d9adf0-82aa-4367-bb60-129f382c646a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"be03a9e5-7a70-45e2-9bd7-6463a9b56a9a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"0a5957e5-5b63-445a-b454-cc38471f8f2b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"d6693f72-ce19-458f-b423-f09b2644ec2a\")>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8916e9a",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the company names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d30e8",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1cf40cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flipkart',\n",
       " 'Accenture',\n",
       " '2coms consulting',\n",
       " 'Okda Solutions',\n",
       " 'Recruise India Consulting Pvt. Ltd.',\n",
       " 'LatentView Analytics Private Limited',\n",
       " 'thinkAPPS Solutions Pvt Ltd',\n",
       " 'PPN Solutions Pvt Ltd.',\n",
       " 'Accenture',\n",
       " 'Accenture']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2f0f6",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience requried data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bc539d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"ac766ab5-cd3d-42e6-805e-09ee90392623\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"d48e805b-c80a-470a-8648-9bcc318d5fd3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"e523a092-b855-42ca-9744-36f0e0c4b869\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"de39b753-57de-4039-bec3-46b16c88bbd1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"82530caa-ddbd-4622-afb6-c9c3f4a6efd5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"29bcdc30-d3b5-4764-9b45-be89cdf1b21a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"7064038c-c4d0-4868-b8ba-14992e9e9fa1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"a39644da-22fa-4944-9121-1bea781d624e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"3b115ed2-88a2-4b12-b9ad-65051de38860\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"0ac6bf3b-fe8c-4d50-949d-7ce65d4841b4\")>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the experience requried data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c5697",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the experience requried data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e0db2",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14d63c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-3 Yrs', '4-6 Yrs', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce170c1",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data. Let me first show you in which tags this data is put on yhe webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad1b6f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"346da8fb-2d3f-426a-8d0c-4c4730f85783\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"108a7396-2205-44a1-aa92-073d61594a93\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"bf8db0bd-0906-49a2-9b63-3a20e34957eb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"3d2b481a-3de5-449f-a7bc-62a2529b25e0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"ec9d3fa9-6bd1-4186-8bf6-43bec5e7aa70\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"bb823a27-83ad-4167-9b50-37d792d82bb6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"db9a4aa7-8987-41b1-b01a-c2666b831ff3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"2a04aec4-d912-4ada-bd44-c7123cb98861\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"aefd2f61-8da5-4916-abb1-55aab0f8d2f9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5f36c1e6381641da377a662f2012192d\", element=\"3e6e80d2-5d5e-4ca7-a1d6-9ca2425a325f\")>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e80573",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the data about the location of job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38430d2",
   "metadata": {},
   "source": [
    "Now we will extract the text(location) from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78401b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru(Bellandur)',\n",
       " 'Bangalore/Bengaluru',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in locations_tags :\n",
    "    location=i.text\n",
    "    locations.append(location)\n",
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425444b",
   "metadata": {},
   "source": [
    "So,now we have extracted the data requried from the webpage and stored them in the 4 lists mentioned above. Now before creating a dataframe from these lists. Lets first check the length of each of the list. Because if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f026291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(company_names),len(experience_list),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbe1a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['experience_requried']=experience_list[0:10]\n",
    "jobs['location']=locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fadbe341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_requried</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst - Flipkart Analytics</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru(Bellandur)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analytics and Interpretation Application ...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manager - Data Analytics</td>\n",
       "      <td>2coms consulting</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analytics Manager - Machine Learning Algo...</td>\n",
       "      <td>Okda Solutions</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analytics - Pharma</td>\n",
       "      <td>Recruise India Consulting Pvt. Ltd.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Manager - Data Analytics</td>\n",
       "      <td>LatentView Analytics Private Limited</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Big Data Developer - Data Analytics</td>\n",
       "      <td>thinkAPPS Solutions Pvt Ltd</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Urgent requirement For Data analytics.</td>\n",
       "      <td>PPN Solutions Pvt Ltd.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S&amp;PP COE Analytics - Data Engineering</td>\n",
       "      <td>Accenture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACN - Digital - Analytics - Big Data - 10</td>\n",
       "      <td>Accenture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                  Data Analyst - Flipkart Analytics   \n",
       "1  Data Analytics and Interpretation Application ...   \n",
       "2                           Manager - Data Analytics   \n",
       "3  Data Analytics Manager - Machine Learning Algo...   \n",
       "4                            Data Analytics - Pharma   \n",
       "5                           Manager - Data Analytics   \n",
       "6                Big Data Developer - Data Analytics   \n",
       "7             Urgent requirement For Data analytics.   \n",
       "8              S&PP COE Analytics - Data Engineering   \n",
       "9          ACN - Digital - Analytics - Big Data - 10   \n",
       "\n",
       "                                company experience_requried  \\\n",
       "0                              Flipkart             0-3 Yrs   \n",
       "1                             Accenture             4-6 Yrs   \n",
       "2                      2coms consulting                       \n",
       "3                        Okda Solutions                       \n",
       "4   Recruise India Consulting Pvt. Ltd.                       \n",
       "5  LatentView Analytics Private Limited                       \n",
       "6           thinkAPPS Solutions Pvt Ltd                       \n",
       "7                PPN Solutions Pvt Ltd.                       \n",
       "8                             Accenture                       \n",
       "9                             Accenture                       \n",
       "\n",
       "                         location  \n",
       "0  Bangalore/Bengaluru(Bellandur)  \n",
       "1             Bangalore/Bengaluru  \n",
       "2                                  \n",
       "3                                  \n",
       "4                                  \n",
       "5                                  \n",
       "6                                  \n",
       "7                                  \n",
       "8                                  \n",
       "9                                  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbd914",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the \n",
    "location” field. \n",
    "3. Then click the search button. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9f6f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "415a6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf79d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e21eed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "607b9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "# job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad34a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"Data Scientist\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84f7daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8af5db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-scientist-jobs-in-bangalore-bengaluru?k=data%20scientist&l=bangalore%2Fbengaluru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4955017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3747e0",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it\n",
    "\n",
    "So,Now lets first create 10 empty lists. In these lists the data will be stored while scraping.We have created 10 empty lists for 4 features which we have to exract\n",
    "\n",
    "1.job_titles2.company_names3.locations_list4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "759795c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47294bbb",
   "metadata": {},
   "source": [
    "First, we will extract all the tags where we have the jobs titles.Let me first show you on the webpage in which tags the job titles are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "023f4fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"d30b507d-2334-4d46-8b72-831b1cfba19d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"9f04e8a0-4eda-4d2a-8cce-e80e2601eef5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"01e41919-1021-467a-a9d4-1644c6726130\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"2374c198-d546-40d6-83a1-d25192f2fa35\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"8031270a-bbae-4037-aaf3-8c49b60ceec4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"8a3babec-9b8f-4940-ab71-11980996934d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"8e211e20-6147-41b9-b636-d94c5eb910cd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"2f5d98da-ba3c-41d4-abaf-8c6e52c458ca\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"ba39d971-77ca-4aee-94a2-1183a61925ff\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"19b5ab7f-25ca-4cb7-aeb9-b5da1bb455c9\")>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the jobs-titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9b4ba",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "032c2ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Data Scientist',\n",
       " 'Data Scientist /Senior Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Senior Data Scientist / Tech Lead - SQL / Python / Big Data',\n",
       " 'Senior / Lead Data Scientist',\n",
       " 'Forecasting Analyst/ Data Scientist (US Client)',\n",
       " 'Data Scientist, Agribusiness Intelligence',\n",
       " 'Data Scientist']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "# so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99713e",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the company names.Let me first show you in which tags the company names are put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81b67d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"807da7ef-c387-4213-9921-48c224b18948\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"78f5f3ee-a33f-49cf-b727-2ceb51bc0d39\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"0d924c26-96e0-414b-9239-7079debfef8d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"2cb06a34-ae1f-4bd1-b486-48a28ec1a676\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"fa6d8562-e547-40cf-a3db-b3f5843ec6cf\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"4566516e-bb56-4eb9-9b03-55e89241b258\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"67c9c8a1-c759-4912-b69b-40e27dbefc3d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"b1eb1d48-3736-4cac-9cc4-9e3355cca41c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"0ce2b77b-6e78-4b8e-8d86-3677ba79c43e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"3bcf27aa-8426-486d-9250-d45db06102a5\")>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5e8d4",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the company names.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bad408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kwalee India Pvt Ltd.',\n",
       " 'Dun & Bradstreet',\n",
       " 'Visa',\n",
       " 'Team4Progress',\n",
       " 'TransOrg Solutions Services (P) Ltd.',\n",
       " 'Exploro Solutions',\n",
       " 'CoreEdge Solutions',\n",
       " 'Concentrix Daksh Services',\n",
       " 'IHS Markit',\n",
       " 'Kwalee India Pvt Ltd.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c94c0a",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience requried data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e0eff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"75096e28-1a17-4378-8d48-1d79d293d4ca\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"9b915bb3-b413-4f8b-88b1-ac7511fdad4a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"742e6687-53d3-4c74-b5d6-dec9d10a6991\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"e76b2b6d-1435-42f6-9b41-dec976fac4d8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"afa5f7b0-d091-4c5c-b037-7057142134bc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"35f18fcb-c2bc-4859-b39d-bfb1047b30d0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"3831b9d1-ce6d-41fc-a05c-c5a8532d5124\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"2015f511-2630-4a86-b477-a97b7ba1fb4c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"2bce2c33-21a7-4812-9cee-906c9aefe221\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"0e6a0952-d3d3-4b7d-920b-a55c11c79827\")>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the experience requried data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0f328",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the experience requried data.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f2a9feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5-10 Yrs',\n",
       " '1-5 Yrs',\n",
       " '4-9 Yrs',\n",
       " '5-8 Yrs',\n",
       " '4-9 Yrs',\n",
       " '5-10 Yrs',\n",
       " '3-8 Yrs',\n",
       " '3-8 Yrs',\n",
       " '3-6 Yrs',\n",
       " '2-7 Yrs']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in experience_tags :\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d617b5",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f017176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"b340c557-a996-4c4c-acb5-71d5cf63dc82\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"0bcffff6-f2aa-4b66-be61-5c844a1fbd06\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"ed3bec97-dd2f-45a6-8030-c9745da6a914\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"35f3b519-3212-406d-95c1-305a15f3c8e6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"20f053e1-28d0-4242-921b-cc23211a71f4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"9bc8b5e5-d5da-48ef-a303-984fde660299\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"40849326-3ee0-48f1-9484-9c52c6e8777e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"5d74aa0a-81ac-458a-8e2a-d11df6b5c8a3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"b47a1c1d-cc5a-4c7d-bd74-a106ff81efbb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"a7579a17b6ec22a243bd86a77ad315cb\", element=\"21f5d87b-2e9c-408c-8f40-0547317f95e7\")>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be693a3c",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the data about the location of job.\n",
    "\n",
    "Now we will extract the text(location) from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d227863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Chennai, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Mumbai, Pune, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'Pune, Chennai, Bangalore/Bengaluru',\n",
       " 'Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru',\n",
       " 'Gurgaon/Gurugram, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in locations_tags :\n",
    "    location=i.text\n",
    "    locations.append(location)\n",
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2f1a0",
   "metadata": {},
   "source": [
    "So,now we have extracted the data requried from the webpage and stored them in the 4 lists mentioned above. Now before creating a dataframe from these lists. Lets first check the length of each of the list. Because if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0f37b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(company_names),len(experience_list),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e04e1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['experience_requried']=experience_list[0:10]\n",
    "jobs['location']=locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e1d3e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_requried</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Kwalee India Pvt Ltd.</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist /Senior Data Scientist</td>\n",
       "      <td>Dun &amp; Bradstreet</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Visa</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Team4Progress</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Mumbai, Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>TransOrg Solutions Services (P) Ltd.</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist / Tech Lead - SQL / Pyth...</td>\n",
       "      <td>Exploro Solutions</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Pune, Chennai, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior / Lead Data Scientist</td>\n",
       "      <td>CoreEdge Solutions</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "      <td>Concentrix Daksh Services</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist, Agribusiness Intelligence</td>\n",
       "      <td>IHS Markit</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kwalee India Pvt Ltd.</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1              Data Scientist /Senior Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3                                Lead Data Scientist   \n",
       "4                                Lead Data Scientist   \n",
       "5  Senior Data Scientist / Tech Lead - SQL / Pyth...   \n",
       "6                       Senior / Lead Data Scientist   \n",
       "7    Forecasting Analyst/ Data Scientist (US Client)   \n",
       "8          Data Scientist, Agribusiness Intelligence   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                company experience_requried  \\\n",
       "0                 Kwalee India Pvt Ltd.            5-10 Yrs   \n",
       "1                      Dun & Bradstreet             1-5 Yrs   \n",
       "2                                  Visa             4-9 Yrs   \n",
       "3                         Team4Progress             5-8 Yrs   \n",
       "4  TransOrg Solutions Services (P) Ltd.             4-9 Yrs   \n",
       "5                     Exploro Solutions            5-10 Yrs   \n",
       "6                    CoreEdge Solutions             3-8 Yrs   \n",
       "7             Concentrix Daksh Services             3-8 Yrs   \n",
       "8                            IHS Markit             3-6 Yrs   \n",
       "9                 Kwalee India Pvt Ltd.             2-7 Yrs   \n",
       "\n",
       "                                            location  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                       Chennai, Bangalore/Bengaluru  \n",
       "2                                Bangalore/Bengaluru  \n",
       "3                  Mumbai, Pune, Bangalore/Bengaluru  \n",
       "4  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...  \n",
       "5                 Pune, Chennai, Bangalore/Bengaluru  \n",
       "6  Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru  \n",
       "7              Gurgaon/Gurugram, Bangalore/Bengaluru  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac13094",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b592e",
   "metadata": {},
   "source": [
    "You have to use the location and salary filter. \n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required. \n",
    "The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs \n",
    "The task will be done as shown in the below steps: \n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field. \n",
    "3. Then click the search button. \n",
    "4. Then apply the location filter and salary filter by checking the respective boxes \n",
    "5. Then scrape the data for the first 10 jobs results you get. \n",
    "6. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8e521ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b32c4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8519d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cee99e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51f50ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4f79b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12827130",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57e75830",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c23fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-scientist-jobs?k=data%20scientist&ctcFilter=3to6&cityTypeGid=9508\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bab1b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d3e98",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver the webpage has opened or not.lets check it\n",
    "\n",
    "So, Now lets first create 10 empty lists.In these lists the data will be stored while scraping.We have created 10 empty lists for 4 features which we have to extract\n",
    "\n",
    "1.job_titles2.company_names3.locations_list4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9b82bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5f5a3",
   "metadata": {},
   "source": [
    "First,we will extract all the tags where we have the jobs titles.Let me first show you on the webpage in which tags the job titles are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88ed234c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"8ed726d1-67e6-4613-9e93-3d81bdb9ae99\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"9830c7c9-927f-4606-a173-41ac34b18c2b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"af7a0349-b123-4871-875e-88199c8a1007\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"29ea2730-63b8-4dd5-a5b2-6da792850cab\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"2a4ae7c6-d3e4-4a4e-b5a9-f3ab87dbea5f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"eff3ffb6-e204-40d2-9f52-2a373ad943f5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"4aff2d56-3a39-471b-b5a8-92d358fba3cc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"bcfc9700-56f1-4956-a4ae-8d312b17ec7e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"baff6cd3-54a7-476c-bec7-4bcba05424b4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"646f241c-5111-40f8-8f69-89548c4632b6\")>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the jobs-titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835dbe38",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dfa706ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist III-2',\n",
       " 'Job Opportunity || Data Scientist || HCL Technologies',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Urgent Hiring || Data Scientist || Delhi',\n",
       " 'Data Scientist',\n",
       " 'Immediate requirement For Data Scientist',\n",
       " 'Data Scientist Internship']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "# so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3bcdf8",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the company names.Let me first show you in which tags the company names are put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "810fdc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"a776b332-b9a4-4f11-bc3d-8be57ad1d467\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"9594e717-5da8-497a-ace8-6d9294895419\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"5b65e7fe-e783-4226-b163-0c9a2197eac7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"3db9f476-0cd9-47c6-af09-9925df4061f3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"9d80b920-35a0-4f13-ab24-ecf971a33068\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"10bf4f98-c683-464e-91e4-93e40fe98a5b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"5d70f854-9829-4e5c-97d6-37834b658196\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"0335c916-10f8-4d5e-acd0-50a22196f2df\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"ec6218dc-b478-41fa-9a26-22164fe8cdf9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"36c1c030-3b65-4afa-ae0f-ff26e7f85fc3\")>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1e32c",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the company names.\n",
    "\n",
    "Now we will extract the text from these tags one by one looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d90bafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think i',\n",
       " 'ThinkBumblebee Analytics Pvt. Ltd.',\n",
       " 'Concentrix Daksh Services',\n",
       " 'HCL Technologies',\n",
       " 'MoMagic Technologies Pvt. Ltd.',\n",
       " 'MoMagic Technologies Pvt. Ltd.',\n",
       " 'Shriram Automall India Limited',\n",
       " 'SVK Global Solutions Private Limited',\n",
       " 'CALIBEHR BUSINESS SUPPORT SERVICES PRIVATE LIMITED',\n",
       " 'iHackers Inc']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd352d",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience requried data.Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19654e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"659e401a-7550-4bcf-83b6-861d8cc5a61e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"9d6ba3ed-a03d-4545-a11b-048ca7366d08\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"b05ab1dd-182f-4333-8974-5c76b63a8f0a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"a48197f6-23ed-4bbc-b47d-ab262d244a49\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"12475402-2371-4d7c-879d-d9280d8829f8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"161ad773-862b-40b5-a4e2-4eb9f3aba7cf\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"78683edb-9b5c-4238-baab-01bff5568965\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"9dd1701c-5a1a-4273-80e1-283ee8edf14b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"289d48b3-3c95-45fb-b5af-8115a1a3c76a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"07900578-d29f-4409-afd7-01b36f28c802\")>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the experience requried data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e052a10",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the experience requried data.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd96d2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-2 Yrs',\n",
       " '2-6 Yrs',\n",
       " '3-8 Yrs',\n",
       " '4-7 Yrs',\n",
       " '4-6 Yrs',\n",
       " '4-6 Yrs',\n",
       " '2-7 Yrs',\n",
       " '6-10 Yrs',\n",
       " '2-7 Yrs',\n",
       " '0-1 Yrs']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee12d8b",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data.Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08a7f53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"9bea782f-6884-4341-85d7-e02e4dfa0447\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"9044eec6-7f73-4003-bf1d-cdc2df85dcd7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"11ce9758-6009-47a4-bb47-f9ac7c0a8eda\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"87826bfe-4f8b-44b3-99db-e7c618b3af0e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"f800d811-51f0-4420-b720-660e5c7ae8b3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"c6c5b860-0815-4e62-b12e-e60550b5a93c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"20c70128-f1de-46fa-9d2b-a530c192e2e6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"481aad1d-caed-4912-9f33-aa46645cc025\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"663fc890-3ccb-40d5-ad70-df9ef17a1ccd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e4157072eec278811a25531319a60bce\", element=\"a777b1fd-cbdb-4b6b-90c8-865c9ab6375c\")>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d788dfa",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the data about the location of job.\n",
    "\n",
    "Now we will extract the text(location) from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41ffcaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kochi/Cochin, Kolkata, Hyderabad/Secunderabad, Pune, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'Pune, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'Gurgaon/Gurugram',\n",
       " 'Delhi / NCR',\n",
       " 'Noida(Sector-126 Noida)',\n",
       " 'Noida(Sector-126 Noida)',\n",
       " 'Delhi / NCR',\n",
       " 'Noida',\n",
       " 'Mumbai, Hyderabad/Secunderabad, Pune, Chennai, Tamia, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'New Delhi']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in locations_tags:\n",
    "    location=i.text\n",
    "    locations.append(location)\n",
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62deb5ea",
   "metadata": {},
   "source": [
    "So,now we have extracted the data requried from the webpage and stored them in the 4 lists mentioned above.Now before creating a dataframe from these lists.Lets first check the length of each of the list. Because if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "524730ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(company_names),len(experience_list),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "331af013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['experience_requried']=experience_list[0:10]\n",
    "jobs['location']=locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2eaf2501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_requried</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Think i</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Kochi/Cochin, Kolkata, Hyderabad/Secunderabad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ThinkBumblebee Analytics Pvt. Ltd.</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Pune, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist III-2</td>\n",
       "      <td>Concentrix Daksh Services</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job Opportunity || Data Scientist || HCL Techn...</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>MoMagic Technologies Pvt. Ltd.</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Noida(Sector-126 Noida)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>MoMagic Technologies Pvt. Ltd.</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Noida(Sector-126 Noida)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Urgent Hiring || Data Scientist || Delhi</td>\n",
       "      <td>Shriram Automall India Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>SVK Global Solutions Private Limited</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Immediate requirement For Data Scientist</td>\n",
       "      <td>CALIBEHR BUSINESS SUPPORT SERVICES PRIVATE LIM...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist Internship</td>\n",
       "      <td>iHackers Inc</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>New Delhi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                     Data Scientist   \n",
       "1                                     Data Scientist   \n",
       "2                               Data Scientist III-2   \n",
       "3  Job Opportunity || Data Scientist || HCL Techn...   \n",
       "4                                     Data Scientist   \n",
       "5                                     Data Scientist   \n",
       "6           Urgent Hiring || Data Scientist || Delhi   \n",
       "7                                     Data Scientist   \n",
       "8           Immediate requirement For Data Scientist   \n",
       "9                          Data Scientist Internship   \n",
       "\n",
       "                                             company experience_requried  \\\n",
       "0                                            Think i             0-2 Yrs   \n",
       "1                 ThinkBumblebee Analytics Pvt. Ltd.             2-6 Yrs   \n",
       "2                          Concentrix Daksh Services             3-8 Yrs   \n",
       "3                                   HCL Technologies             4-7 Yrs   \n",
       "4                     MoMagic Technologies Pvt. Ltd.             4-6 Yrs   \n",
       "5                     MoMagic Technologies Pvt. Ltd.             4-6 Yrs   \n",
       "6                     Shriram Automall India Limited             2-7 Yrs   \n",
       "7               SVK Global Solutions Private Limited            6-10 Yrs   \n",
       "8  CALIBEHR BUSINESS SUPPORT SERVICES PRIVATE LIM...             2-7 Yrs   \n",
       "9                                       iHackers Inc             0-1 Yrs   \n",
       "\n",
       "                                            location  \n",
       "0  Kochi/Cochin, Kolkata, Hyderabad/Secunderabad,...  \n",
       "1             Pune, Bangalore/Bengaluru, Delhi / NCR  \n",
       "2                                   Gurgaon/Gurugram  \n",
       "3                                        Delhi / NCR  \n",
       "4                            Noida(Sector-126 Noida)  \n",
       "5                            Noida(Sector-126 Noida)  \n",
       "6                                        Delhi / NCR  \n",
       "7                                              Noida  \n",
       "8  Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...  \n",
       "9                                          New Delhi  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ce2a9",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes: \n",
    "1. Brand \n",
    "2. Product Description \n",
    "3. Price \n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4bf9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f1ac0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1090a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7e1fc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91f72718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for product search bar\n",
    "product = driver.find_element_by_class_name(\"_3704LK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "49cf9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "product.send_keys(\"sunglasses\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "177bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb83002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96f511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730be2b",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it\n",
    "\n",
    "So,Now lets first create 100 empty lists. In these lists the data will be stored while scraping.We have created 100 empty lists for 3 features which we have to exract\n",
    "\n",
    "1.Brand2.Description3.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7d1bb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "Description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "990c3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements_by_class_name('_2WkVRV')\n",
    "#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)\n",
    "#appending the text in Brand list\n",
    "        prices=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "#appending the text in price list\n",
    "    desc=driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "#scraping description from the xpath\n",
    "    for i in desc:\n",
    "        description.append(i.text)\n",
    "#appending the text in description list\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6782f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c29cee7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (56)</td>\n",
       "      <td>₹188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SHAAH COLLECTIONS</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Rectangular...</td>\n",
       "      <td>₹177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>Night Vision, Riding Glasses Wayfarer Sunglass...</td>\n",
       "      <td>₹198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (53)</td>\n",
       "      <td>₹319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>elegante</td>\n",
       "      <td>UV Protection Oval Sunglasses (Free Size)</td>\n",
       "      <td>₹359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Brand                                        Description Price\n",
       "0            Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹513\n",
       "1                SRPM             UV Protection Wayfarer Sunglasses (56)  ₹188\n",
       "2           Elligator                UV Protection Round Sunglasses (54)  ₹248\n",
       "3              PIRASO              UV Protection Aviator Sunglasses (54)  ₹200\n",
       "4   SHAAH COLLECTIONS  UV Protection, Polarized, Mirrored Rectangular...  ₹177\n",
       "..                ...                                                ...   ...\n",
       "95     ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...  ₹399\n",
       "96             GANSTA  Night Vision, Riding Glasses Wayfarer Sunglass...  ₹198\n",
       "97          Elligator             UV Protection Wayfarer Sunglasses (53)  ₹319\n",
       "98     ROZZETTA CRAFT  UV Protection Retro Square Sunglasses (Free Size)  ₹499\n",
       "99           elegante          UV Protection Oval Sunglasses (Free Size)  ₹359\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff99aa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2020.12.5)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.4.7)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (20.0.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225406a",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC \n",
    "TSVZAXUHGREPBFGI&marketplace. When you will open the above link you will reach to the below shown webpage ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94a32a",
   "metadata": {},
   "source": [
    "As shown in the above page you have to scrape the tick marked attributes. These are: \n",
    "1. Rating \n",
    "2. Review_summary \n",
    "3. Full review \n",
    "4. You have to scrape this data for first 100 reviews. \n",
    "Note: All the steps required during scraping should be done through code only and not manually. \n",
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the \n",
    "search field. \n",
    "You have to scrape 4 attributes of each sneaker: \n",
    "1. Brand \n",
    "2. Product Description \n",
    "3. Price \n",
    "As shown in the below image, you have to scrape the tick marked attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97648f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23adc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ddd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d03ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for product search bar\n",
    "product = driver.find_element_by_class_name(\"_3704LK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "product.send_keys(\"sneakers\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8989b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9869f",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it\n",
    "\n",
    "So,Now lets first create 100 empty lists. In these lists the data will be stored while scraping.We have created 100 empty lists for 3 features which we have to exract\n",
    "\n",
    "1.brand2.description3.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41821eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    #for loop for scrapping 4 page\n",
    "    \n",
    "    brands=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    #scraping title tags by xpath\n",
    "    for i in brands:\n",
    "        brand.append(i.text)\n",
    "        #appending the text in brand\n",
    "        \n",
    "        price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "        #scraping the price from the xpath\n",
    "        for i in price_tags:\n",
    "            price.append(i.text)\n",
    "            #appending the text in price tags\n",
    "            \n",
    "            description=driver.find_elements_by_xpath(\"//span[@class='B_NuCI']\")\n",
    "            #scraping the description from the xpath\n",
    "            for i in description:\n",
    "                description.append(i.text)\n",
    "                #appending the text in description\n",
    "                \n",
    "                nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "                #scraping the list of buttons from the page\n",
    "                try:\n",
    "                    driver.get(nxt_button[1].get_attribute('href'))\n",
    "                 #getting the link from the list for next page \n",
    "                except:\n",
    "                    driver.get(nxt_button[0].get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be34fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'brand':brand[:100],\n",
    "                'description':'description'[:100],\n",
    "                'price':price[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b750b457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
